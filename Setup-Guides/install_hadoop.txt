Install Apache Hadoop:
======================

sudo apt update

sudo apt-get install openjdk-8-jdk

nano ~/.bashrc

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin

source ~/.bashrc

Step 2: Create the SSH Key for password-less login (Press enter button when it asks you to enter a filename to save the key)

sudo apt-get install openssh-server openssh-client

ssh-keygen -t rsa -P ""

Copy the generated ssh key to authorized keys

cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

ssh localhost

exit


Step 3: Download the Hadoop 3.2.1 Package/Binary file


mkdir -p workarea

cd workarea


https://hadoop.apache.org/releases.html

Choose hadoop version(3.2.4)

wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz

tar -xzvf hadoop-3.2.4.tar.gz

Step 4: Add the HADOOP_HOME and JAVA_HOME paths in the bash file (.bashrc)

nano ~/.bashrc


# HADOOP VARIABLES SETTINGS START HERE

export HADOOP_HOME=/home/xuanbach14052002/workarea/hadoop-3.2.4
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export HADOOP_OPTS="-Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR"

# HADOOP VARIABLES SETTINGS END HERE


source ~/.bashrc


hadoop version


Step 5: Create or Modifiy Hadoop configuration files

Now create/edit the configuration files in /home/xuanbach14052002/workarea/hadoop-3.2.4/etc/hadoop directory.

Edit hadoop-env.sh as follows,

nano /home/xuanbach14052002/workarea/hadoop-3.2.4/etc/hadoop/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64


mkdir -p /home/xuanbach14052002/workarea/hadoop_data/tmp

Edit core-site.xml as follows,

nano /home/xuanbach14052002/workarea/hadoop-3.2.4/etc/hadoop/core-site.xml

	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/xuanbach14052002/workarea/hadoop_data/tmp</value>
		<description>Parent directory for other temporary directories.</description>
	</property>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://0.0.0.0:9000</value>
		<description>The name of the default file system. </description>
	</property>


mkdir -p /home/xuanbach14052002/workarea/hadoop_data/namenode

mkdir -p /home/xuanbach14052002/workarea/hadoop_data/datanode

sudo chown -R xuanbach14052002:xuanbach14052002 /home/xuanbach14052002/workarea


Edit hdfs-site.xml as follows,

nano /home/xuanbach14052002/workarea/hadoop-3.2.4/etc/hadoop/hdfs-site.xml

	<property>
		<name>dfs.namenode.name.dir</name>
		<value>/home/xuanbach14052002/workarea/hadoop_data/namenode</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>/home/xuanbach14052002/workarea/hadoop_data/datanode</value>
	</property>
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
	

Edit mapred-site.xml as follows,

nano /home/xuanbach14052002/workarea/hadoop-3.2.4/etc/hadoop/mapred-site.xml

	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>

Edit yarn-site.xml as follows,

nano /home/xuanbach14052002/workarea/hadoop-3.2.4/etc/hadoop/yarn-site.xml

	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address</name>
		<value>0.0.0.0:8088</value>
	</property>


Setting ownership for /home/xuanbach14052002/workarea

sudo chown -R xuanbach14052002:xuanbach14052002 /home/xuanbach14052002/workarea


Step 6: Format the namenode

hdfs namenode -format

hdfs datanode -format


Start the NameNode daemon and DataNode daemon by using the scripts in the /sbin directory, provided by Hadoop.

cd /home/xuanbach14052002/workarea/hadoop-3.2.4/sbin

start-dfs.sh


Start ResourceManager daemon and NodeManager daemon.


start-yarn.sh


Run jps command to check running hadoop JVM processes

jps

FYI.

~/workarea/hadoop-3.2.4/etc/hadoop$ jps
22289 Jps
21969 NodeManager
21609 SecondaryNameNode
21418 DataNode
21819 ResourceManager
21277 NameNode


Open your web browser and go to the below URL to browse the NameNode.

http://localhost:9870

hdfs dfs -mkdir /Lakehouse
hdfs dfs -mkdir /Lakehouse/Bronze
hdfs dfs -mkdir /Lakehouse/Silver
hdfs dfs -mkdir /Lakehouse/Checkpoint_Bronze
hdfs dfs -mkdir /Lakehouse/Checkpoint_Silver